{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Generation using LSTM for Sherlock Holmes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, import libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # avoid parallelism warning\n",
        "\n",
        "# to do: try to use tensorflow-metal instead of base tf\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import platform\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "print('Python version:', platform.python_version())\n",
        "print('Tensorflow version:', tf.__version__)\n",
        "print('Keras version:', tf.keras.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wandb.integration.keras import WandbMetricsLogger\n",
        "\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import data + directory for saving checkpoints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = open(dataset_file_path, mode='r').read()\n",
        "print(text[:250])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from transformers import GPT2Tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we tokenize the file. The total vocabulary size will be 30000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = ByteLevelBPETokenizer()\n",
        "save_dir=\"./tokenizer\"\n",
        "\n",
        "# Train tokenizer if folder is empty\n",
        "if (not os.path.exists(save_dir)) or (len(os.listdir(save_dir)) == 0):\n",
        "    tokenizer.train(files=[dataset_file_path], vocab_size=30_000, min_frequency=2)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    tokenizer.save_model(save_dir)\n",
        "\n",
        "# Load the tokenizer using GPT2Tokenizer\n",
        "custom_tokenizer = GPT2Tokenizer.from_pretrained(save_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenize the text file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids = custom_tokenizer.encode(text)\n",
        "print(f\"Total tokens in text: {len(input_ids)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepping data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens_dataset = tf.data.Dataset.from_tensor_slices(input_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequence_length = 100\n",
        "examples_per_epoch = len(input_ids) // (sequence_length + 1)\n",
        "\n",
        "print(f'Examples per epoch: {examples_per_epoch}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate batched sequences out of the token dataset\n",
        "sequences = tokens_dataset.batch(sequence_length + 1, drop_remainder=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split sequences into input and target\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show some examples of input-target pairs\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    for i in range(5):\n",
        "        if i < len(input_example):\n",
        "            input_token = input_example[i].numpy()\n",
        "            target_token = target_example[i].numpy()\n",
        "            print(f'Step {i:2d}')\n",
        "            print(f'  input token: {input_token} ({custom_tokenizer.decode([input_token])})')\n",
        "            print(f'  expected output token: {target_token} ({custom_tokenizer.decode([target_token])})')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch size.\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Batched dataset size: {}'.format(len(list(dataset.as_numpy_iterator()))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get vocabulary size from tokenizer\n",
        "vocab_size = custom_tokenizer.vocab_size + 1  # +1 for the padding token if added\n",
        "\n",
        "# Model hyperparams\n",
        "EMBED_DIM = 256\n",
        "MODEL_DIM = 256\n",
        "NUM_HEADS = 8\n",
        "FF_DIM = 1024\n",
        "NUM_LAYERS = 4\n",
        "DROPOUT = 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def positional_encoding(length: int, depth: int) -> tf.Tensor:\n",
        "    pos = tf.range(length, dtype=tf.float32)[:, tf.newaxis]\n",
        "    idx = tf.range(depth, dtype=tf.float32)[tf.newaxis, :]\n",
        "    angle_rates = 1 / tf.pow(10000.0, (2 * (idx // 2)) / tf.cast(depth, tf.float32))\n",
        "    angles = pos * angle_rates\n",
        "    sines = tf.math.sin(angles[:, 0::2])\n",
        "    cosines = tf.math.cos(angles[:, 1::2])\n",
        "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "    return pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "\n",
        "def transformer_block(x: tf.Tensor, model_dim: int, ff_dim: int, num_heads: int, dropout: float) -> tf.Tensor:\n",
        "    attn_out = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=model_dim, dropout=dropout)(x, x)\n",
        "    attn_out = tf.keras.layers.Dropout(dropout)(attn_out)\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + attn_out)\n",
        "\n",
        "    ff = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(model_dim),\n",
        "    ])\n",
        "    ff_out = ff(x)\n",
        "    ff_out = tf.keras.layers.Dropout(dropout)(ff_out)\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + ff_out)\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_model(vocab_size, embed_dim, model_dim, num_heads, ff_dim, num_layers, dropout):\n",
        "    inputs = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    emb = tf.keras.layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "    proj = tf.keras.layers.Dense(model_dim)(emb)\n",
        "    pos = positional_encoding(length=sequence_length, depth=model_dim)\n",
        "\n",
        "    def add_positional(x):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        return x + pos[:, :seq_len, :]\n",
        "\n",
        "    x = tf.keras.layers.Lambda(add_positional)(proj)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        x = transformer_block(x, model_dim, ff_dim, num_heads, dropout)\n",
        "\n",
        "    logits = tf.keras.layers.Dense(vocab_size)(x)\n",
        "    model = tf.keras.Model(inputs, logits, name=\"token_transformer\")\n",
        "    return model\n",
        "\n",
        "model = build_model(vocab_size, EMBED_DIM, MODEL_DIM, NUM_HEADS, FF_DIM, NUM_LAYERS, DROPOUT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
        "        y_true=labels,\n",
        "        y_pred=logits,\n",
        "        from_logits=True\n",
        "    )\n",
        "\n",
        "# Compile the model\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss=loss\n",
        ")\n",
        "\n",
        "# Directory for checkpoints\n",
        "checkpoint_dir = cache_dir + '/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Checkpoint filename\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'model_epoch_{epoch:02d}.weights.h5')\n",
        "\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "from resource_monitor import ResourceMonitorCB\n",
        "monitor_cb = ResourceMonitorCB(monitor_interval=2.0)\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='loss', patience=3, restore_best_weights=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run = wandb.init(project='my-first-project', group='transformer-experiment')\n",
        "\n",
        "EPOCHS = 2  # TESTING\n",
        "history = model.fit(\n",
        "    x=dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[\n",
        "        checkpoint_cb,\n",
        "        early_stopping,\n",
        "        WandbMetricsLogger(),\n",
        "        monitor_cb\n",
        "    ]\n",
        ")\n",
        "\n",
        "run.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def render_training_history(training_history):\n",
        "    loss = training_history.history['loss']\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(loss, label='Training set')\n",
        "    plt.legend()\n",
        "    plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n",
        "    plt.show()\n",
        "\n",
        "render_training_history(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load best model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "# Get list of all weight files\n",
        "weight_files = glob.glob(os.path.join(checkpoint_dir, '*.weights.h5'))\n",
        "\n",
        "if weight_files:\n",
        "    latest = max(weight_files, key=os.path.getctime)  # or sort by name, etc.\n",
        "    model.load_weights(latest)\n",
        "    print(f\"Loaded weights from: {latest}\")\n",
        "else:\n",
        "    print(\"No weights file found. Using randomly initialized model.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, start_string, num_generate=300, temperature=1.0):\n",
        "    # Tokenize the start string\n",
        "    input_ids = custom_tokenizer.encode(start_string)\n",
        "    input_ids = tf.expand_dims(input_ids, 0)\n",
        "\n",
        "    generated_tokens = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        logits = model(input_ids)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        predicted_id = tf.random.categorical(logits, num_samples=1)[:, -1].numpy()[0]\n",
        "        generated_tokens.append(predicted_id)\n",
        "        input_ids = tf.concat([input_ids, tf.expand_dims([[predicted_id]], 0)[:, 0, :]], axis=1)\n",
        "\n",
        "    return start_string + custom_tokenizer.decode(generated_tokens)\n",
        "\n",
        "print(generate_text(model, start_string=\"Sherlock Holmes looked at the \", num_generate=300, temperature=1.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experiments on grammar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import language_tool_python\n",
        "\n",
        "def evaluate_grammar_quality(generated_text):\n",
        "\n",
        "    try:\n",
        "        tool = language_tool_python.LanguageTool('en-GB')\n",
        "    except:\n",
        "        print(\"LanguageTool not available. Installing via pip...\")\n",
        "        import subprocess\n",
        "        import sys\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"language-tool-python\"])\n",
        "        tool = language_tool_python.LanguageTool('en-GB')\n",
        "    \n",
        "    # Find grammar and spelling errors\n",
        "    matches = tool.check(generated_text)\n",
        "    \n",
        "    # Count errors by category\n",
        "    error_categories = Counter()\n",
        "    for match in matches:\n",
        "        error_categories[match.category] = error_categories.get(match.category, 0) + 1\n",
        "    \n",
        "    # Calculate word count\n",
        "    words = re.findall(r'\\b\\w+\\b', generated_text)\n",
        "    word_count = len(words)\n",
        "    \n",
        "    # Calculate errors per 100 words\n",
        "    errors_per_100_words = (len(matches) / max(1, word_count)) * 100\n",
        "    \n",
        "    results = {\n",
        "        'total_errors': len(matches),\n",
        "        'error_categories': dict(error_categories),\n",
        "        'errors_per_100_words': errors_per_100_words,\n",
        "        'word_count': word_count\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "def merge_error_categories(error_cat_list):\n",
        "    merged = Counter()\n",
        "    for error_cat in error_cat_list:\n",
        "        for category, count in error_cat.items():\n",
        "            merged[category] += count\n",
        "    \n",
        "    # Calculate averages\n",
        "    result = {category: count / len(error_cat_list) for category, count in merged.items()}\n",
        "    return result\n",
        "\n",
        "\n",
        "def run_evaluations(model, start_string, num_runs=5, num_tokens=300, temperatures=[0.7, 1.0]):\n",
        "    all_results = {}\n",
        "    \n",
        "    for temp in temperatures:\n",
        "        print(f\"\\nEvaluating temperature: {temp}\")\n",
        "        run_results = []\n",
        "        \n",
        "        for run in range(1, num_runs + 1):\n",
        "            print(f\"  Run {run}/{num_runs}...\")\n",
        "            text = generate_text(model, start_string, num_tokens, temp)\n",
        "            run_results.append(evaluate_grammar_quality(text))\n",
        "        \n",
        "        # Calculate averages across runs\n",
        "        avg_results = {\n",
        "            'total_errors': sum(r['total_errors'] for r in run_results) / num_runs,\n",
        "            'errors_per_100_words': sum(r['errors_per_100_words'] for r in run_results) / num_runs,\n",
        "            'word_count': sum(r['word_count'] for r in run_results) / num_runs,\n",
        "            'error_categories': merge_error_categories([r['error_categories'] for r in run_results])\n",
        "        }\n",
        "        \n",
        "        # Store individual runs for reference\n",
        "        avg_results['individual_runs'] = run_results\n",
        "        all_results[temp] = avg_results\n",
        "    \n",
        "    return all_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define temperatures to test\n",
        "temperatures = [1.0]\n",
        "start_string = \"Sherlock Holmes looked at the\"\n",
        "\n",
        "# Run the evaluations 5 times for each temperature\n",
        "results = run_evaluations(model, start_string, num_runs=5, num_tokens=300, temperatures=temperatures)\n",
        "\n",
        "print(\"\\n===== AVERAGED RESULTS =====\")\n",
        "for temp, result in results.items():\n",
        "    print(f\"\\nTemperature: {temp}\")\n",
        "    print(f\"Average total errors: {result['total_errors']:.2f}\")\n",
        "    print(f\"Average errors per 100 words: {result['errors_per_100_words']:.2f}\")\n",
        "    print(f\"Average word count: {result['word_count']:.2f}\")\n",
        "    print(\"Average error categories:\")\n",
        "    \n",
        "    # Sort categories by frequency for better readability\n",
        "    sorted_categories = sorted(\n",
        "        result['error_categories'].items(), \n",
        "        key=lambda x: x[1], \n",
        "        reverse=True\n",
        "    )\n",
        "    \n",
        "    for category, avg_count in sorted_categories:\n",
        "        print(f\"  - {category}: {avg_count:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cache_dir = './tmp-transformer'\n",
        "dataset_file_name = 'sherlockholmes.txt'\n",
        "\n",
        "dataset_file_path = dataset_file_name\n",
        "\n",
        "print(dataset_file_path)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
